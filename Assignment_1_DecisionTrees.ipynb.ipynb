{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='chap-tparbresdecision'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Decision Trees\n",
    "\n",
    "The objective of this assignment is to demonstrate the implementation of decision trees for classification and regression problems. This document freely reproduces some examples shown in the excellent scikit-learn documentation.\n",
    "\n",
    "Useful external references\n",
    "\n",
    "- [NumPy documentation](https://docs.scipy.org/doc/numpy/user/index.html)  \n",
    "- [SciPy documentation](https://docs.scipy.org/doc/scipy/reference/)  \n",
    "- [MatPlotLib documentation](http://matplotlib.org/)  \n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html)  \n",
    "- [Python Programming languages](https://www.python.org)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees\n",
    "\n",
    "Decision trees are nonparametric learning methods used for classification and regression problems. The goal is to create a model that predicts the values of the target variable, based on a set of sequences of decision rules inferred from the training data. The tree therefore approximates the target by a succession of if-then-else rules. This paradigm applies to both categorical and numerical data. The more complex the tree generated, the better the model “explains” the learning data but the higher the risk of over-fitting. \n",
    "\n",
    "Decision trees have several advantages that make them interesting in contexts where it is useful to understand the sequence of decisions made by the model:\n",
    "\n",
    "•\tThey are easy to understand and visualize.\n",
    "\n",
    "•\tThey require little data preparation (normalization, etc.).\n",
    "\n",
    "•\tThe cost of using trees is logarithmic.\n",
    "\n",
    "•\tThey can use categorical and numerical data.\n",
    "\n",
    "•\tThey can deal with multi-class problems.\n",
    "\n",
    "•\tWhite box model: the result is easy to conceptualize and visualize.\n",
    "\n",
    "However, these models have two major disadvantages:\n",
    "\n",
    "•\tOver-fitting: sometimes the generated trees are too complex and generalize badly. Choosing good values for the maximum depth (max_depth) and minimum number of samples per leaf (min_samples_leaf) parameters avoids this problem.\n",
    "\n",
    "•\tIt may happen that the generated trees are not balanced, which implies that the travel time is no longer logarithmic. It is therefore recommended to adjust the dataset before construction, to avoid that one class largely dominates the others in terms of the number of training examples\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trees for classification\n",
    "\n",
    "In scikit-learn, the class [sklearn.tree.DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) allows to perform a multi-class classification using a decision tree.\n",
    "\n",
    "We start by importing the right modules and building the tree object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in c:\\users\\ayoub\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.14.6 in c:\\users\\ayoub\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from scikit-learn) (1.16.5)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in c:\\users\\ayoub\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from scikit-learn) (0.13.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.1.0 in c:\\users\\ayoub\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in c:\\users\\ayoub\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from scikit-learn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "# To install the last version of sklearn\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example, we can define a minimalist dataset (two points, each in a class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree is built using the `.fit(X, y)` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction on new samples is done in the usual way with `.predict(X)` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "clf.predict([[2., 2.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also predict the probability of each class for a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "clf.predict_proba([[2., 2.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of Iris data\n",
    "\n",
    "`DecisionTreeClassifier` is able to handle multi-class classification problems (eg with labels 0, 1, … K-1). In this example we will work with the [Iris](https://archive.ics.uci.edu/ml/datasets/Iris) dataset, , easily accessible in `sklearn`. This dataset contains 150 instances of iris (a type of plant, each observation describes its morphology). The objective is to classify each instance into one of three categories: *Iris setosa*, *Iris virginica* or *Iris versicolor*.\n",
    "\n",
    "One of the classes is linearly separable with respect to the other two, but the other two are not separable with respect to each other.\n",
    "\n",
    "\n",
    "<dl style='margin: 20px 0;'>\n",
    "<dt>The attributes of the dataset are:</dt>\n",
    "<dd>\n",
    "- sepal length,  \n",
    "- sepal width,  \n",
    "- petal length,  \n",
    "- petal width, \n",
    "- class: Iris Setosa, Iris Versicolor or Iris Virginica. \n",
    "\n",
    "\n",
    "</dd>\n",
    "\n",
    "</dl>\n",
    "\n",
    "The Iris dataset being very common, scikit-learn offers a native function to load it into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Calculate the statistics (mean and standard deviation) of the four explanatory variables: sepal length, sepal width, petal length and petal width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "How many examples of each class are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the model, let's split the dataset into two: 70% for training, 30% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build a decision tree on this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training is finished, we can visualize the tree, either with  `matplotlib` using the `plot_tree` method, or with the  `graphviz` tool (dot command). For example, with  `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "tree.plot_tree(clf, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, it is possible to export by producing a `.dot` file which is the default format of `graphviz`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Export the graph to the iris.dot file\n",
    "with open(\"iris.dot\", 'w') as f:\n",
    "    f = tree.export_graphviz(clf, out_file=f, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, on the command line, it is possible to convert this file into many formats, for example into PDF (shell command):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "Currently, this command does not work on CNAM's JupyterHub because dot is not installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "dot -Tpdf iris.dot -o iris.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated image should look like this:\n",
    "\n",
    " <img src=\"https://scikit-learn.org/stable/_images/iris.png\" style=\"width:100%;\">\n",
    "\n",
    "\n",
    "Once the model is built, it is possible to use it for prediction on new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test score can be calculated this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3:\n",
    "\n",
    "Change the `max_depth` and `min_samples_leaf` parameter values. What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4:\n",
    "\n",
    "The problem here being particularly simple, redo a training/test division with 5% of the data in training and 95% test. \n",
    "Calculate the rate of misclassified items on the test set. \n",
    "Vary (or better, perform a grid search with `GridSearchCV`) the values of the `max_depth` and `min_samples_leaf` parameters to measure their impact on this score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Display of the decision surface\n",
    "\n",
    "For a pair of attributes, i.e., for two-dimensional observations, we can visualize the decision surface in 2 dimensions. First, we discretize the two-dimensional domain with a constant step and then we evaluate the model on each point of the grid.\n",
    "\n",
    "\n",
    "In this example, we only keep the length and width of the petals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Settings\n",
    "n_classes = 3\n",
    "plot_colors = \"bry\" # blue-red-yellow\n",
    "plot_step = 0.02\n",
    "\n",
    "# Choose the length and width attributes of the petals\n",
    "pair = [2, 3]\n",
    "\n",
    "# We only keep the two attributes\n",
    "X = iris.data[:, pair]\n",
    "y = iris.target\n",
    "\n",
    "# Tree learning\n",
    "clf = tree.DecisionTreeClassifier().fit(X, y)\n",
    "\n",
    "# Display of the decision surface\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "plt.xlabel(iris.feature_names[pair[0]])\n",
    "plt.ylabel(iris.feature_names[pair[1]])\n",
    "plt.axis(\"tight\")\n",
    "\n",
    "# Display of learning points\n",
    "for i, color in zip(range(n_classes), plot_colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i], cmap=plt.cm.Paired)\n",
    "plt.axis(\"tight\")\n",
    "plt.suptitle(\"Decision surface of a decision tree using paired features\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5:\n",
    "\n",
    "Redo the display for the other pairs of attributes. On which pair is the separation between the classes the most marked?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \tDecision trees for regression\n",
    "\n",
    "For regression with decision trees, scikit-learn offers the `DecisionTreeRegressor` class. As for the classification, the  `fit(...)` takes as input the parameter `X` (attributes of the observations). Warning: the `y` are not class labels but real values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "X = [[0, 0], [2, 2]]\n",
    "y = [0.5, 2.5]\n",
    "clf = tree.DecisionTreeRegressor()\n",
    "clf = clf.fit(X, y)\n",
    "clf.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we will construct a sinusoidal signal affected by white noise and we will train a regression tree on this training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create the training data\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "fig.add_subplot(121)\n",
    "plt.plot(X, y)\n",
    "plt.title(\"Pure sine wave\")\n",
    "\n",
    "# We add a random noise every 5 samples\n",
    "y[::5] += 3 * (0.5 - np.random.rand(16))\n",
    "fig.add_subplot(122)\n",
    "plt.plot(X, y)\n",
    "plt.title(\"Noisy sine wave\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to regress this signal `y` from the values of `x`. For this, we use a regression tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Learn the model\n",
    "reg = DecisionTreeRegressor(max_depth=2)\n",
    "reg.fit(X, y)\n",
    "\n",
    "# Prediction on the same range of values\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# Display of results\n",
    "plt.figure()\n",
    "plt.scatter(X, y, c=\"darkorange\", label=\"Training Examples\")\n",
    "plt.plot(X_test, y_pred, color=\"cornflowerblue\", label=\"Prediction\", linewidth=2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6:\n",
    "\n",
    "Change the value of the `max_depth` parameter. What happens if we take too large a value? Too small? Change the rate of elements affected by noise (the `y[::5]`). When all elements are affected by noise, should a high or low value for `max_depth` be preferred?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7:\n",
    "\n",
    "To deepen, load the Diabetes dataset from the `sklearn.datasets` module and make a random partition into learning part and test part (70% learning, 30% testing). Build a regression tree model on this basis. Calculate the root mean square error on the test set. Do a grid search to find the `max_depth` parameter value that minimizes this error."
   ]
  }
 ],
 "metadata": {
  "date": 1663157799.3671956,
  "filename": "tpArbresDecision.rst",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "title": "Travaux pratiques - Arbres de décision"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
